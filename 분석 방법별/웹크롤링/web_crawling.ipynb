{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    body {\n",
       "        --vscode-font-family: \"KoddiUD 온고딕\"\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "    body {\n",
    "        --vscode-font-family: \"KoddiUD 온고딕\"\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹 크롤링\n",
    "- 인터넷 상에 존재하는 데이터를 자동으로 수집하는 행위\n",
    "- 데이터 분석가에게 데이터를 탐색하고 원하는 조건에 맞는 데이터를 직접 수집하고 저장까지 하기 위한 목적으로 사용\n",
    "1. 웹 페이지 정보 가져오기\n",
    "  - 파이썬 Requests 라이브러리 사용\n",
    "2. HTML 소스를 파싱(분석)하여 원하는 정보 얻기\n",
    "  - 파이썬 BeautifulSoup 라이브러리 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 개념 알기\n",
    "1. 사용자는 브라우저로 접속하고 싶은 주소(URL) 입력\n",
    "2. 브라우저가 해당 주소의 서버에게 페이지 구성 정보를 달라고 요청(request)\n",
    "3. 웹 서버는 구성에 필요한 정보를 코드(html) 형태로 전달(response)\n",
    "4. 브라우저는 서버가 전달해준 정보(html)을 해석해서 사용자 화면에 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 웹 데이터 수집 라이브러리(BeautifulSoup)\n",
    "- HTML 및 XML에서 데이터를 쉽게 처리하는 파이썬 라이브러리\n",
    "- HTML은 태그로 이루어져 있고, 수많은 공백과 변화하는 소스들 때문에 오류가 있을 가능성이 높지만 BeautifulSoup를 이용하면 이러한 오류를 잡아서 고친 후 데이터를 전달해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설치 방법\n",
    "# pip install beautifulsoup4\n",
    "\n",
    "# 사용 방법\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup HTML 코드 작성\n",
    "# HTML 코드 작성\n",
    "html =  \"\"\"\n",
    "<html>\n",
    "\t<body>\n",
    "\t<h1 id = 'title'>파이썬 라이브러리 활용!</h1>\n",
    "\t<p id = 'body'>오늘의 주제는 웹 데이터 수집</p>\n",
    "\t<p class = 'scraping'>삼성전자 일별 시세 불러오기</p>\n",
    "\t</body>\n",
    "<html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 BeautifulSoup HTML 파싱\n",
    "- soup = BeautifulSoup(html, 'html.parser')\n",
    "  - html을 파이썬에서 읽을 수 있게 파싱(파이썬 객체로 변환)\n",
    "  - html이라는 변수에 저장한 html 소스코드를 .parser를 붙여 변환\n",
    "  - parser는 파이썬의 내장 메소드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 BeautifulSoup 데이터를 텍스트로 반환\n",
    "- `for text in soup:`\n",
    "    - `print(text)`\n",
    "- soup\n",
    "  - soup의 데이터를 모두 가져와서 텍스트로 반환\n",
    "- soup.contents \n",
    "  - soup의 데이터를 모두 가져와서 리스트로 변환\n",
    "- soup.stripped_strings\n",
    "  - 공백도 함께 제거하여 텍스트로 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<html>\n",
      "<body>\n",
      "<h1 id=\"title\">파이썬 라이브러리 활용!</h1>\n",
      "<p id=\"body\">오늘의 주제는 웹 데이터 수집</p>\n",
      "<p class=\"scraping\">삼성전자 일별 시세 불러오기</p>\n",
      "</body>\n",
      "<html>\n",
      "</html></html>\n"
     ]
    }
   ],
   "source": [
    "for text in soup:\n",
    "  print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soup.stripped_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이썬 라이브러리 활용!\n",
      "오늘의 주제는 웹 데이터 수집\n",
      "삼성전자 일별 시세 불러오기\n"
     ]
    }
   ],
   "source": [
    "for stripped_text in soup.stripped_strings:\n",
    "  print(stripped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 BeautifulSoup Find 함수\n",
    "- `scraping = soup.find(class_='scraping')`\n",
    "  - `scraping.string`\n",
    "- find 함수는 id, class, element 등을 검색하는 기능\n",
    "  - find\n",
    "    - 조건에 해당하는 첫번째 정보만 검색\n",
    "    - 클래스의 이름을 알 경우, `class_` 형태로 사용\n",
    "  - find_all\n",
    "    - 조건에 해당하는 모든 정보 검색\n",
    "  - string\n",
    "    - 태그 내부의 텍스트만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\">파이썬 라이브러리 활용!</h1>\n"
     ]
    }
   ],
   "source": [
    "# find 함수 활용\n",
    "title = soup.find(id='title')\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"scraping\">삼성전자 일별 시세 불러오기</p>\n"
     ]
    }
   ],
   "source": [
    "# scraping 조건에 해당하는 첫 번째 정보만 검색\n",
    "scraping = soup.find(class_='scraping')\n",
    "print(scraping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'삼성전자 일별 시세 불러오기'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraping.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 BeautifulSoup 웹 크롤링 3단계 과정\n",
    "- Request\n",
    "  - 웹 페이지의 URL 이용해서 HTML 문서를 요청\n",
    "- Response\n",
    "  - 요청한 HTML 문서를 회신\n",
    "- Parsing\n",
    "  - 태그 기반으로 파싱(일련의 문자열을 의미있는 단위로 분해)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹 페이지의 링크를 이용해서 HTML 문서를 요청하기 위해 필요한 라이브러리\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 **Request**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) URL 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL 저장\n",
    "# stock_url이라는 변수에 URL 저장\n",
    "stock_url = 'https://finance.naver.com//item/sise_day.naver?code=005930'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. User-agent 설정\n",
    "   - user-agent 확인 사이트\n",
    "     - http://www.useragentstring.com/\n",
    "   - 웹 크롤링을 진행하면 종종 페이지에서 아무것도 받아오지 못하는 경우가 발생함\n",
    "     - 대부분의 서버에서 봇을 차단하기 때문\n",
    "   - 그래서 user-agent를 headers에 저장하면 오류를 해결할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header에 user-agent 값 저장\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. requests.get()\n",
    "  - 웹 페이지의 URL 이용해서 HTML 문서를 요청\n",
    "    - `requests.get(stock_url, headers = header)`\n",
    "    - URL 값을 파라미터 값으로 입력\n",
    "    - 해당 사이트는 반드시 헤더 정보를 요구하기 때문에 파라미터 값으로 헤더 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(stock_url, headers = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 **Response**\n",
    "- 요청한 HTML 문서를 회신\n",
    "- `response = requests.get(stock_url, headers = headers)`\n",
    "  - 서버에서 요청을 받아 처리한 후, 요청자에게 응답 줌\n",
    "  - HTML 코드 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response 변수에 요청한 HTML 문서를 회신하여 저장\n",
    "response = requests.get(stock_url, headers = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 **Parsing**\n",
    "- 태그 기반으로 파싱(일련의 문자열을 의미있는 단위로 분해)\n",
    "- `soup = BeautifulSoup(response.text, 'html.parser')`\n",
    "  - html을 파이썬에서 읽을 수 있게 파싱\n",
    "  - response.text에 저장한 html 소스코드를 .parser를 붙여 변환\n",
    "  - parser는 파이썬의 내장 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup 변수에 BeautifulSoup의 객체 생성\n",
    "# HTML 코드를 파이썬에서 읽을 수 있도록 파싱\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 BeautifulSoup 반복문으로 일별 종가 구현\n",
    "1. 200일 일별 종가 정보는 1 page 당 10일의 일별 종가 정보가 담겨 있어서 20page 필요\n",
    "2. 일별 종가 담긴 URL과 Header 정보로 requests.get 함수 구현\n",
    "3. 요청한 HTML 문서를 회신하여 response 변수에 저장\n",
    "4. BeautifulSoup 함수로 HTML을 읽을 수 있도록 soup 변수에 저장\n",
    "5. page 개수만큼 20번 반복\n",
    "   1. 'tr' 태그 조건에 해당하는 모든 정보 검색하여 parsing_list 변수에 저장\n",
    "6. 한 페이지 당 10일의 일별 종가 정보가 담겨있어서 10번 반복\n",
    "   1. 'td' 태그의 align가 'center'인 값들 중에서 0번째 조건에 해당하는 정보를 검색하여 출력\n",
    "   2. 'td' 태그의 class가 'num'인 값들 중 0번째 조건에 해당하는 정보 검색하여 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2024.12.06 54,100\n",
      "2024.12.05 53,700\n",
      "2024.12.04 53,100\n",
      "2024.12.03 53,600\n",
      "2024.12.02 53,600\n",
      "2024.11.29 54,200\n",
      "2024.11.28 55,500\n",
      "2024.11.27 56,300\n",
      "2024.11.26 58,300\n",
      "2024.11.25 57,900\n",
      "2\n",
      "2024.11.22 56,000\n",
      "2024.11.21 56,400\n",
      "2024.11.20 55,300\n",
      "2024.11.19 56,300\n",
      "2024.11.18 56,700\n",
      "2024.11.15 53,500\n",
      "2024.11.14 49,900\n",
      "2024.11.13 50,600\n",
      "2024.11.12 53,000\n",
      "2024.11.11 55,000\n",
      "3\n",
      "2024.11.08 57,000\n",
      "2024.11.07 57,500\n",
      "2024.11.06 57,300\n",
      "2024.11.05 57,600\n",
      "2024.11.04 58,700\n",
      "2024.11.01 58,300\n",
      "2024.10.31 59,200\n",
      "2024.10.30 59,100\n",
      "2024.10.29 59,600\n",
      "2024.10.28 58,100\n",
      "4\n",
      "2024.10.25 55,900\n",
      "2024.10.24 56,600\n",
      "2024.10.23 59,100\n",
      "2024.10.22 57,700\n",
      "2024.10.21 59,000\n",
      "2024.10.18 59,200\n",
      "2024.10.17 59,700\n",
      "2024.10.16 59,500\n",
      "2024.10.15 61,000\n",
      "2024.10.14 60,800\n",
      "5\n",
      "2024.10.11 59,300\n",
      "2024.10.10 58,900\n",
      "2024.10.08 60,300\n",
      "2024.10.07 61,000\n",
      "2024.10.04 60,600\n",
      "2024.10.02 61,300\n",
      "2024.09.30 61,500\n",
      "2024.09.27 64,200\n",
      "2024.09.26 64,700\n",
      "2024.09.25 62,200\n",
      "6\n",
      "2024.09.24 63,200\n",
      "2024.09.23 62,600\n",
      "2024.09.20 63,000\n",
      "2024.09.19 63,100\n",
      "2024.09.13 64,400\n",
      "2024.09.12 66,300\n",
      "2024.09.11 64,900\n",
      "2024.09.10 66,200\n",
      "2024.09.09 67,500\n",
      "2024.09.06 68,900\n",
      "7\n",
      "2024.09.05 69,000\n",
      "2024.09.04 70,000\n",
      "2024.09.03 72,500\n",
      "2024.09.02 74,400\n",
      "2024.08.30 74,300\n",
      "2024.08.29 74,000\n",
      "2024.08.28 76,400\n",
      "2024.08.27 75,800\n",
      "2024.08.26 76,100\n",
      "2024.08.23 77,700\n",
      "8\n",
      "2024.08.22 78,300\n",
      "2024.08.21 78,300\n",
      "2024.08.20 78,900\n",
      "2024.08.19 78,300\n",
      "2024.08.16 80,200\n",
      "2024.08.14 77,200\n",
      "2024.08.13 76,100\n",
      "2024.08.12 75,500\n",
      "2024.08.09 74,700\n",
      "2024.08.08 73,400\n",
      "9\n",
      "2024.08.07 74,700\n",
      "2024.08.06 72,500\n",
      "2024.08.05 71,400\n",
      "2024.08.02 79,600\n",
      "2024.08.01 83,100\n",
      "2024.07.31 83,900\n",
      "2024.07.30 81,000\n",
      "2024.07.29 81,200\n",
      "2024.07.26 80,900\n",
      "2024.07.25 80,400\n",
      "10\n",
      "2024.07.24 82,000\n",
      "2024.07.23 83,900\n",
      "2024.07.22 83,000\n",
      "2024.07.19 84,400\n",
      "2024.07.18 86,900\n",
      "2024.07.17 86,700\n",
      "2024.07.16 87,700\n",
      "2024.07.15 86,700\n",
      "2024.07.12 84,400\n",
      "2024.07.11 87,600\n",
      "11\n",
      "2024.07.10 87,800\n",
      "2024.07.09 87,800\n",
      "2024.07.08 87,400\n",
      "2024.07.05 87,100\n",
      "2024.07.04 84,600\n",
      "2024.07.03 81,800\n",
      "2024.07.02 81,800\n",
      "2024.07.01 81,800\n",
      "2024.06.28 81,500\n",
      "2024.06.27 81,600\n",
      "12\n",
      "2024.06.26 81,300\n",
      "2024.06.25 80,800\n",
      "2024.06.24 80,600\n",
      "2024.06.21 80,000\n",
      "2024.06.20 81,600\n",
      "2024.06.19 81,200\n",
      "2024.06.18 79,800\n",
      "2024.06.17 78,100\n",
      "2024.06.14 79,600\n",
      "2024.06.13 78,600\n",
      "13\n",
      "2024.06.12 76,500\n",
      "2024.06.11 75,200\n",
      "2024.06.10 75,700\n",
      "2024.06.07 77,300\n",
      "2024.06.05 77,400\n",
      "2024.06.04 75,300\n",
      "2024.06.03 75,700\n",
      "2024.05.31 73,500\n",
      "2024.05.30 73,500\n",
      "2024.05.29 75,200\n",
      "14\n",
      "2024.05.28 77,600\n",
      "2024.05.27 77,200\n",
      "2024.05.24 75,900\n",
      "2024.05.23 78,300\n",
      "2024.05.22 77,700\n",
      "2024.05.21 78,400\n",
      "2024.05.20 78,900\n",
      "2024.05.17 77,400\n",
      "2024.05.16 78,200\n",
      "2024.05.14 78,300\n",
      "15\n",
      "2024.05.13 78,400\n",
      "2024.05.10 79,200\n",
      "2024.05.09 79,700\n",
      "2024.05.08 81,300\n",
      "2024.05.07 81,300\n",
      "2024.05.03 77,600\n",
      "2024.05.02 78,000\n",
      "2024.04.30 77,500\n",
      "2024.04.29 76,700\n",
      "2024.04.26 76,700\n",
      "16\n",
      "2024.04.25 76,300\n",
      "2024.04.24 78,600\n",
      "2024.04.23 75,500\n",
      "2024.04.22 76,100\n",
      "2024.04.19 77,600\n",
      "2024.04.18 79,600\n",
      "2024.04.17 78,900\n",
      "2024.04.16 80,000\n",
      "2024.04.15 82,200\n",
      "2024.04.12 83,700\n",
      "17\n",
      "2024.04.11 84,100\n",
      "2024.04.09 83,600\n",
      "2024.04.08 84,500\n",
      "2024.04.05 84,500\n",
      "2024.04.04 85,300\n",
      "2024.04.03 84,100\n",
      "2024.04.02 85,000\n",
      "2024.04.01 82,000\n",
      "2024.03.29 82,400\n",
      "2024.03.28 80,800\n",
      "18\n",
      "2024.03.27 79,800\n",
      "2024.03.26 79,900\n",
      "2024.03.25 78,200\n",
      "2024.03.22 78,900\n",
      "2024.03.21 79,300\n",
      "2024.03.20 76,900\n",
      "2024.03.19 72,800\n",
      "2024.03.18 72,800\n",
      "2024.03.15 72,300\n",
      "2024.03.14 74,300\n",
      "19\n",
      "2024.03.13 74,100\n",
      "2024.03.12 73,300\n",
      "2024.03.11 72,400\n",
      "2024.03.08 73,300\n",
      "2024.03.07 72,200\n",
      "2024.03.06 72,900\n",
      "2024.03.05 73,700\n",
      "2024.03.04 74,900\n",
      "2024.02.29 73,400\n",
      "2024.02.28 73,200\n",
      "20\n",
      "2024.02.27 72,900\n",
      "2024.02.26 72,800\n",
      "2024.02.23 72,900\n",
      "2024.02.22 73,100\n",
      "2024.02.21 73,000\n",
      "2024.02.20 73,300\n",
      "2024.02.19 73,800\n",
      "2024.02.16 72,800\n",
      "2024.02.15 73,000\n",
      "2024.02.14 74,000\n"
     ]
    }
   ],
   "source": [
    "# 200일 동안의 일별 종가 정보 가져오는 반복문(1페이지 당 10일 정보 담겨있음)\n",
    "for page in range(1, 21):\n",
    "  print (str(page))\n",
    "\n",
    "  # url + page 번호 합치기\n",
    "  stock_url = 'http://finance.naver.com/item/sise_day.nhn?code=005930' +'&page='+ str(page)\n",
    "\n",
    "  # header 정보\n",
    "  headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "\n",
    "  # request : 웹 페이지의 URL, header 이용해서 HTML 문서 요청\n",
    "  # response : 요청한 HTML 문서 회신\n",
    "  response = requests.get(stock_url, headers = headers)\n",
    "\n",
    "  # parsing : HTML을 읽을 수 있도록 파싱\n",
    "  # soup 변수에 BeautifulSoup의 객체 생성\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "  # \"tr\" 태그 조건에 해당하는 모든 정보 검색\n",
    "  parsing_list = soup.find_all(\"tr\")\n",
    "\n",
    "  # None 값은 걸러주기 위한 변수 생성\n",
    "  isCheckNone = None\n",
    "\n",
    "  # 페이지당 일별 종가 출력하기 위한 반복문 <들여쓰기 주의>\n",
    "  for i in range(1, len(parsing_list)):\n",
    "    # None 값은 걸러주기 위한 조건문 <들여쓰기 주의>\n",
    "    # .span()는 매치된 문자열의 (시작, 끝)에 해당하는 튜플을 돌려주는 함수\n",
    "    if(parsing_list[i].span != isCheckNone):\n",
    "      # parsing_list[i] : i번째 parsing_list, i 번째 \"tr\" 태그 값\n",
    "      # .find_all(\"td\", align=\"center\")[0].text : \"td\" 태그의 align가 \"center\"인 값들 중 0번째 값\n",
    "      # .find_all(\"td\", class_=\"num\")[0].text : \"td\" 태그의 class가 \"num\"인 값들 중 0번째 값\n",
    "      print(parsing_list[i].find_all(\"td\", align=\"center\")[0].text,\n",
    "            parsing_list[i].find_all(\"td\", class_=\"num\")[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Pandas 일별 시세 테이블 구현\n",
    "1) Pandas 라이브러리와 Requests 라이브러리 이용\n",
    "2) 200일 일별 종가 정보는 1 Page 당 10일의 일별 종가 정보 담겨있어서 20 Page 필요\n",
    "3) 일별 종가 담긴 URL과 Header 정보로 requests.get 함수 구현\n",
    "4) pandas.read_html 함수를 통해 HTML 불러와서 파싱\n",
    "5) concat 함수를 이용하여 dataframe 끝에 추가하고 싶은 요소를 추가하여 dataframe 리턴\n",
    "6) dropna 함수를 통해 결측 값 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n",
      "/var/folders/yl/lyd4cgp16bj2jd049cmb64n00000gn/T/ipykernel_32162/4231495009.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_page = pd.read_html(response.text, header=0)[0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>날짜</th>\n",
       "      <th>종가</th>\n",
       "      <th>전일비</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>거래량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024.12.06</td>\n",
       "      <td>54100.0</td>\n",
       "      <td>상승  400</td>\n",
       "      <td>53900.0</td>\n",
       "      <td>54400.0</td>\n",
       "      <td>52700.0</td>\n",
       "      <td>22805072.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024.12.05</td>\n",
       "      <td>53700.0</td>\n",
       "      <td>상승  600</td>\n",
       "      <td>53200.0</td>\n",
       "      <td>54400.0</td>\n",
       "      <td>53200.0</td>\n",
       "      <td>23588277.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024.12.04</td>\n",
       "      <td>53100.0</td>\n",
       "      <td>하락  500</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>53400.0</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>29004766.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024.12.03</td>\n",
       "      <td>53600.0</td>\n",
       "      <td>보합0</td>\n",
       "      <td>53100.0</td>\n",
       "      <td>54400.0</td>\n",
       "      <td>53100.0</td>\n",
       "      <td>23374603.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024.12.02</td>\n",
       "      <td>53600.0</td>\n",
       "      <td>하락  600</td>\n",
       "      <td>54300.0</td>\n",
       "      <td>54400.0</td>\n",
       "      <td>53100.0</td>\n",
       "      <td>22044867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>2024.02.20</td>\n",
       "      <td>73300.0</td>\n",
       "      <td>하락  500</td>\n",
       "      <td>73700.0</td>\n",
       "      <td>73700.0</td>\n",
       "      <td>72800.0</td>\n",
       "      <td>14681477.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>2024.02.19</td>\n",
       "      <td>73800.0</td>\n",
       "      <td>상승  1,000</td>\n",
       "      <td>72800.0</td>\n",
       "      <td>73900.0</td>\n",
       "      <td>72800.0</td>\n",
       "      <td>12726404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>2024.02.16</td>\n",
       "      <td>72800.0</td>\n",
       "      <td>하락  200</td>\n",
       "      <td>73300.0</td>\n",
       "      <td>73400.0</td>\n",
       "      <td>72500.0</td>\n",
       "      <td>13444781.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>2024.02.15</td>\n",
       "      <td>73000.0</td>\n",
       "      <td>하락  1,000</td>\n",
       "      <td>74200.0</td>\n",
       "      <td>74400.0</td>\n",
       "      <td>73000.0</td>\n",
       "      <td>14120600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>2024.02.14</td>\n",
       "      <td>74000.0</td>\n",
       "      <td>하락  1,200</td>\n",
       "      <td>73700.0</td>\n",
       "      <td>74300.0</td>\n",
       "      <td>73700.0</td>\n",
       "      <td>12434945.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             날짜       종가        전일비       시가       고가       저가         거래량\n",
       "1    2024.12.06  54100.0    상승  400  53900.0  54400.0  52700.0  22805072.0\n",
       "2    2024.12.05  53700.0    상승  600  53200.0  54400.0  53200.0  23588277.0\n",
       "3    2024.12.04  53100.0    하락  500  52000.0  53400.0  52000.0  29004766.0\n",
       "4    2024.12.03  53600.0        보합0  53100.0  54400.0  53100.0  23374603.0\n",
       "5    2024.12.02  53600.0    하락  600  54300.0  54400.0  53100.0  22044867.0\n",
       "..          ...      ...        ...      ...      ...      ...         ...\n",
       "294  2024.02.20  73300.0    하락  500  73700.0  73700.0  72800.0  14681477.0\n",
       "295  2024.02.19  73800.0  상승  1,000  72800.0  73900.0  72800.0  12726404.0\n",
       "296  2024.02.16  72800.0    하락  200  73300.0  73400.0  72500.0  13444781.0\n",
       "297  2024.02.15  73000.0  하락  1,000  74200.0  74400.0  73000.0  14120600.0\n",
       "298  2024.02.14  74000.0  하락  1,200  73700.0  74300.0  73700.0  12434945.0\n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 네이버 금융 일별 시세 테이블 불러오기\n",
    "# pandas 라이브러리와 requests 라이브러리 이용\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "# 빈 데이터프레임을 생성하여 이후 각 페이지에서 가져온 데이터를 추가\n",
    "stock_data = pd.DataFrame()\n",
    "\n",
    "# code = 회사 코드, page = 일별 시세 테이블의 페이지 수 (200 행의 데이터 불러오려면 20 페이지 입력)\n",
    "for page in range(1, 21): # ➊ 페이지 순회\n",
    "   stock_url = 'http://finance.naver.com/item/sise_day.nhn?code=005930' +'&page='+ str(page)\n",
    "\n",
    "   #  header 정보\n",
    "   headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "\n",
    "   # request : 웹 페이지의 URL, header 이용해서 HTML 문서 요청\n",
    "   # response : 요청한 HTML 문서 회신\n",
    "   response = requests.get(stock_url, headers = headers)\n",
    "\n",
    "\n",
    "   # ➋ response.text로 응답을 주면 HTML 코드이기 때문에 read_html로 불러오기\n",
    "   df_page = pd.read_html(response.text, header=0)[0]\n",
    "\n",
    "\n",
    "   # ➌ concat() : 여러 DataFrame을 하나로 결합할 때 사용\n",
    "   # 주의!! : pandas 2.0.0 버전 이후부터 'append()' Method가 완전히 제거되었기 때문에 더 이상 작동하지 않습니다. concat() 사용(설명 아래 참고)\n",
    "   stock_data = pd.concat([stock_data, df_page], ignore_index=True)\n",
    "\n",
    "\n",
    "# ➍ 결측값 행 제거(for문 외부에 위치하도록 들여쓰기 주의)\n",
    "stock_data = stock_data.dropna()\n",
    "stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
